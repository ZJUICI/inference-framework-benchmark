
# dolphin-2.6-mistral-7b-dpo-laser

## A100 40G

## vllm 0.2.7
> vllm v0.3.0  Testing has revealed some issues with the usage of KV cache, where a near 99% usage leads to excessive request wait times.
> so use 0.2.7 instead

### Launching Command
```sh
python3 -m vllm.entrypoints.api_server --model <model dir> --port 8080 --disable-log-requests
```
| Input Length | Output Length | Throughput (requests/s) | Throughput (output token/s) | Average latency per token (ms) | Average latency per output token (ms) |
| :----------: | :-----------: | :---------------------: | :-------------------------: | :----------------------------: | :-----------------------------------: |
|     128      |      128      |          13.86          |           1774.72           |              7.20              |                 14.40                 |
|     128      |      512      |          4.34           |           2224.24           |             11.67              |                 14.59                 |
|     128      |     1024      |          1.84           |           1889.09           |             13.10              |                 14.74                 |
|     128      |     2048      |          0.67           |           1366.80           |             13.96              |                 14.83                 |
|     512      |      128      |          9.34           |           1196.00           |              3.04              |                 15.19                 |
|     512      |      512      |          3.00           |           1538.49           |              7.52              |                 15.03                 |
|     512      |     1024      |          1.42           |           1458.92           |             10.02              |                 15.03                 |
|     512      |     2048      |          0.58           |           1178.69           |             12.04              |                 15.04                 |
|     1024     |      128      |          5.83           |           745.97            |              1.75              |                 15.72                 |
|     1024     |      512      |          2.24           |           1148.80           |              5.06              |                 15.19                 |
|     1024     |     1024      |          1.10           |           1123.25           |              7.58              |                 15.15                 |
|     1024     |     2048      |          0.47           |           963.67            |             10.12              |                 15.19                 |
|     2048     |      128      |          3.24           |           414.92            |              0.98              |                 16.60                 |
|     2048     |      512      |          1.38           |           704.78            |              3.09              |                 15.47                 |
|     2048     |     1024      |          0.73           |           745.78            |              5.13              |                 15.38                 |
|     2048     |     2048      |          0.34           |           689.70            |              7.68              |                 15.36                 |

## tgi v1.4.0
> I plan to rebuild the 'text-generation-router' binary for this benchmark because the original 'tgi router' does not support the 'ignore-eos-token' parameter. However, we need to use it in this benchmark to calculate both the 'per token latency' and the 'per output token latency'.

### Launching Command

```sh
text-generation-launcher --model-id <model name or path> --port 8080  --max-batch-prefill-tokens 4096 --max-input-length 4096 --max-total-tokens 8192 --max-concurrent-requests 1024
```
#### without `ignore_eos_token`, use generate tokens to calculate some metrics
| Input Length | Output Length | Throughput (requests/s) | Throughput (output token/s) | Average latency per token (ms) | Average latency per output token (ms) |
| :----------: | :-----------: | :---------------------: | :-------------------------: | :----------------------------: | :-----------------------------------: |
|     128      |      128      |          16.04          |           1755.69           |              7.79              |                 15.58                 |
|     128      |      512      |          7.63           |           2496.72           |             11.47              |                 15.25                 |
|     128      |     1024      |          4.22           |           2178.47           |              9.33              |                 15.42                 |
|     128      |     2048      |          2.59           |           1777.16           |             14.02              |                 15.78                 |
|     512      |      128      |          8.97           |           925.44            |              3.18              |                 15.92                 |
|     512      |      512      |          6.21           |           1378.38           |              7.93              |                 15.87                 |
|     512      |     1024      |          4.56           |           1500.23           |              2.61              |                 17.01                 |
|     512      |     2048      |          3.26           |           1359.23           |              8.80              |                 16.12                 |
|     1024     |      128      |          5.00           |           582.59            |              1.81              |                 16.29                 |
|     1024     |      512      |          3.31           |           1185.19           |              5.20              |                 15.73                 |
|     1024     |     1024      |          2.27           |           1352.49           |              0.32              |                 20.81                 |
|     1024     |     2048      |          1.65           |           1158.72           |              8.47              |                 16.24                 |
|     2048     |      128      |          2.60           |           321.15            |              1.01              |                 17.25                 |
|     2048     |      512      |          1.82           |           774.19            |              3.32              |                 16.62                 |
|     2048     |     1024      |          1.32           |           938.73            |              3.44              |                 16.09                 |
|     2048     |     2048      |          0.91           |           882.78            |              3.31              |                 16.83                 |


#### with `ignore_eos_token` , still running


### Benchmark Command
```sh
# `obo-num` represents the number of one-by-one requests sent
# `num-prompts` without `requests rate`, this will send all requests simultaneously.
python3 -m benchmark.benchmark_serving --backend {tgi|vllm} --port 8080 --dataset ./ShareGPT_V3_unfiltered_cleaned_split.json  --tokenizer <model name or path> --obo-num 10 --input-lengths 128,512,1024,2048 --num-prompts 388 --output ./output/output.md --request-rate 20
```



## Throughput (Requests/s)
![image](./%20Throughput%20(requests_s)%20.png)
## Throughput (Output Token/s)
![image](./%20Throughput%20(output%20token_s)%20.png)
## Average Latency Per token/(ms)
![image](./%20Average%20latency%20per%20token%20(ms)%20.png)
## Average Latency Per Output Token(ms)
![image](.//%20Average%20latency%20per%20output%20token%20(ms)%20.png)